{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "'''\n",
    "    Model : bert-base\n",
    "    Dataset : Rotten Tomato Movie Review Dataset\n",
    "    Info : 0 -> Negative / 1 -> Positive\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bfa9b",
   "metadata": {},
   "source": [
    "##### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea243c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20ef68",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7070635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\", \n",
    "#                                   origin=URL,\n",
    "#                                   untar=True,\n",
    "#                                   cache_dir='.',\n",
    "#                                   cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The shutil module offers a number of high-level operations on files and collections of files.\n",
    "# import os\n",
    "# import shutil\n",
    "# # Create main directory path (\"/aclImdb\")\n",
    "# main_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "# # Create sub directory path (\"/aclImdb/train\")\n",
    "# train_dir = os.path.join(main_dir, 'train')\n",
    "# # Remove unsup folder since this is a supervised learning task\n",
    "# remove_dir = os.path.join(train_dir, 'unsup')\n",
    "# shutil.rmtree(remove_dir)\n",
    "# # View the final train folder\n",
    "# print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='training', seed=123)\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='validation', seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773dc51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i in train.take(1):\n",
    "  train_feat = i[0].numpy()\n",
    "  train_lab = i[1].numpy()\n",
    "\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "train.columns = ['text', 'label']\n",
    "train['text'] = train['text'].str.decode(\"utf-8\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in test.take(1):\n",
    "  test_feat = j[0].numpy()\n",
    "  test_lab = j[1].numpy()\n",
    "\n",
    "test = pd.DataFrame([test_feat, test_lab]).T\n",
    "test.columns = ['text', 'label']\n",
    "test['text'] = test['text'].str.decode(\"utf-8\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1b8cb",
   "metadata": {},
   "source": [
    "# Loading pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b576f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Tkenizer and Model (bert-base-uncased)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402870d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394da86d",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5577c3",
   "metadata": {},
   "source": [
    "#### Steps for preparing data : \n",
    "#### 1) Filtering / removing un-necessary words/tokens from dataset\n",
    "#### 2) Tokenization\n",
    "#### 3) Padding\n",
    "#### 4) Positional Embeddings / Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da30a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd07a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13eb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5afa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "def convert_data_to_examples(train, test, data, label):\n",
    "    train_input_examples = train.apply(lambda x: InputExample(\n",
    "        guid = None,\n",
    "        text_a = x[data],\n",
    "        text_b = None,\n",
    "        label = x[label]\n",
    "    ), axis=1)\n",
    "    test_input_examples = test.apply(lambda x: InputExample(\n",
    "        guid = None,\n",
    "        text_a = x[data],\n",
    "        text_b = None,\n",
    "        label = x[label]\n",
    "    ), axis=1)\n",
    "    \n",
    "    \n",
    "    return train_input_examples, test_input_examples\n",
    "\n",
    "\n",
    "train_input_examples, test_input_examples = convert_data_to_examples(train, test, 'text', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "#     features = []\n",
    "    \n",
    "#     for example in examples:\n",
    "#         input_dict = tokenizer.encode_plus(\n",
    "#             example.text_a,\n",
    "#             add_special_tokens = True,\n",
    "#             max_length = max_length,\n",
    "#             return_token_type_ids = True,\n",
    "#             return_attention_mask = True,\n",
    "#             pad_to_max_length = True,\n",
    "#             truncation = True\n",
    "#         )\n",
    "# #         method 'input_dict' returns a dictionary with following keys\n",
    "#         input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n",
    "    \n",
    "#         features.append(\n",
    "#             InputFeatures(\n",
    "#                 input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=example.label\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#         def gen():\n",
    "#             for f in features:\n",
    "#                 yield(\n",
    "#                     {\n",
    "#                         \"input_ids\":f.input_ids,\n",
    "#                         \"token_type_ids\":f.token_type_ids,\n",
    "#                         \"attention_mask\":f.attention_mask,\n",
    "#                     },\n",
    "#                     f.label,\n",
    "#                 )\n",
    "                \n",
    "#         return tf.data.Dataset.from_generator(\n",
    "#             gen,\n",
    "#             ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "#             (\n",
    "#                 {\n",
    "#                     \"input_ids\": tf.TensorShape([None]),\n",
    "#                     \"attention_mask\": tf.TensorShape([None]),\n",
    "#                     \"token_type_ids\": tf.TensorShape([None]),\n",
    "#                 },\n",
    "#                 tf.TensorShape([]),\n",
    "#             ),\n",
    "#         ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1338b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"My name is rajesh.\"\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed096b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "    \n",
    "    for example in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            list(example.text_a),\n",
    "            add_special_tokens = True,\n",
    "            max_length = max_length,\n",
    "            return_token_type_ids = True,\n",
    "            return_attention_mask = True,\n",
    "            pad_to_max_length = True,\n",
    "            truncation = True\n",
    "        )\n",
    "#         method 'input_dict' returns a dictionary with following keys\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n",
    "    \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=example.label\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        def gen():\n",
    "            for f in features:\n",
    "                yield(\n",
    "                    {\n",
    "                        \"input_ids\":f.input_ids,\n",
    "                        \"token_type_ids\":f.token_type_ids,\n",
    "                        \"attention_mask\":f.attention_mask,\n",
    "                    },\n",
    "                    f.label,\n",
    "                )\n",
    "                \n",
    "        return tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "            (\n",
    "                {\n",
    "                    \"input_ids\": tf.TensorShape([None]),\n",
    "                    \"attention_mask\": tf.TensorShape([None]),\n",
    "                    \"token_type_ids\": tf.TensorShape([None]),\n",
    "                },\n",
    "                tf.TensorShape([]),\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_examples_to_tf_dataset(list(train_input_examples),tokenizer)\n",
    "\n",
    "test_data = convert_examples_to_tf_dataset(list(test_input_examples),tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.cardinality())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda42913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in train_data:\n",
    "    print('Input IDs:', data['input_ids'])\n",
    "    print('Attention Mask:', data['attention_mask'])\n",
    "    print('Token Type IDs:', data['token_type_ids'])\n",
    "    print('Labels:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79933cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for _ in train_data:\n",
    "    count += 1\n",
    "\n",
    "print(\"Dataset Size:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9554f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31cfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b048c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data is now ready to be fed into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9bdaac",
   "metadata": {},
   "source": [
    "# Select a model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration of our model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "model.compile(optimizer=Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[SparseCategoricalAccuracy('accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, epochs=2, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9f3af",
   "metadata": {},
   "source": [
    "# Fine tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeca1c0",
   "metadata": {},
   "source": [
    "# Test (test.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fdb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = ['I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played Thunderbirds before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82374f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs = model(tf_batch)\n",
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "  print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b852177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42c6a64",
   "metadata": {},
   "source": [
    "# Deployment (AWS/Gradio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
